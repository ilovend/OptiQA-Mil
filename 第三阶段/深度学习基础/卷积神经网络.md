# 卷积神经网络学习笔记 — 框架

说明：本文件为学习卷积神经网络的笔记框架，后续在对应小节补充要点、示例、习题与心得。

## 目录
- [卷积神经网络概述](#卷积神经网络概述)
- [卷积层原理](#卷积层原理)
- [池化层](#池化层)
- [经典CNN架构](#经典cnn架构)
- [现代CNN架构](#现代cnn架构)
- [CNN可视化与解释](#cnn可视化与解释)
- [迁移学习](#迁移学习)
- [CNN训练技巧](#cnn训练技巧)
- [轻量化网络](#轻量化网络)
- [CNN在光学检测中的应用](#cnn在光学检测中的应用)
- [工业级CNN实现](#工业级cnn实现)
- [学习资源与书签](#学习资源与书签)
- [学习进度与 TODO 列表](#学习进度与-todo-列表)

---

## 卷积神经网络概述
- CNN的基本原理
- CNN与全连接网络的区别
- 局部感受野与参数共享
- 平移不变性
- CNN发展历史与里程碑
- 示例：
```python
# CNN结构可视化示例
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.utils import plot_model

# 创建一个简单的CNN模型
model = Sequential([
    Conv2D(6, kernel_size=(5, 5), padding='valid', activation='relu', 
           input_shape=(32, 32, 1), name='conv1'),
    MaxPooling2D(pool_size=(2, 2), name='pool1'),
    Conv2D(16, kernel_size=(5, 5), activation='relu', name='conv2'),
    MaxPooling2D(pool_size=(2, 2), name='pool2'),
    Flatten(name='flatten'),
    Dense(120, activation='relu', name='fc1'),
    Dense(84, activation='relu', name='fc2'),
    Dense(10, activation='softmax', name='output')
])

# 打印模型结构
model.summary()

# 可视化模型结构(需要安装graphviz和pydot)
# pip install graphviz pydot
try:
    plot_model(model, to_file='lenet5_model.png', show_shapes=True, show_layer_names=True)
    print("模型结构图已保存为 lenet5_model.png")
except Exception as e:
    print(f"模型可视化失败: {e}")
    print("请确保已安装graphviz和pydot")

# 可视化卷积核学习过程(假设数据)
fig, axes = plt.subplots(3, 3, figsize=(10, 10))
for i, ax in enumerate(axes.flat):
    # 生成随机卷积核进行演示
    if i < 8:
        # 模拟不同训练阶段的卷积核
        kernel = np.random.randn(5, 5) * (i+1)/10
        im = ax.imshow(kernel, cmap='viridis')
        ax.set_title(f'第1层卷积核 #{i+1}\n(训练中)')
    else:
        ax.axis('off')
        
plt.colorbar(im, ax=axes.ravel().tolist(), shrink=0.8)
plt.tight_layout()
plt.show()

# 特征图激活可视化(假设数据)
fig, axes = plt.subplots(2, 4, figsize=(12, 6))
for i, ax in enumerate(axes.flat):
    # 模拟不同层的特征图
    if i < 8:
        # 生成随机特征图用于演示
        size = 32 // (2 ** (i//4 + 1))
        feature_map = np.random.rand(size, size) * (i+1)/4
        im = ax.imshow(feature_map, cmap='inferno')
        layer_type = "卷积层" if i % 2 == 0 else "池化层"
        layer_num = i//2 + 1
        ax.set_title(f'{layer_type} {layer_num}\n特征图 #{i%4+1}')
        ax.axis('off')
        
plt.colorbar(im, ax=axes.ravel().tolist(), shrink=0.8)
plt.tight_layout()
plt.show()
```
- 记录：
    - CNN核心思想理解：

## 卷积层原理
- 卷积操作数学原理
- 卷积核与滤波器
- 步长与填充
- 通道与特征图
- 1x1卷积的作用
- 记录：
    - 卷积计算细节：

## 池化层
- 最大池化
- 平均池化
- 全局池化
- 池化的作用
- 无池化架构趋势
- 记录：
    - 池化方法比较：

## 经典CNN架构
- LeNet-5
- AlexNet
- VGG
- GoogleNet与Inception模块
- ResNet与残差连接
- 记录：
    - 经典架构演进：

## 现代CNN架构
- Inception系列演进
- ResNeXt
- DenseNet
- EfficientNet
- Vision Transformer与CNN混合
- 记录：
    - 现代架构特点：

## CNN可视化与解释
- 卷积层滤波器可视化
- 激活图分析
- 类激活映射(CAM)
- Grad-CAM
- 对抗性示例
- 记录：
    - 可视化工具与方法：

## 迁移学习
- 预训练模型使用
- 特征提取
- 微调策略
- 领域适应
- 小样本学习
- 记录：
    - 迁移学习最佳实践：

## CNN训练技巧
- 数据增强技术
- 批归一化
- 学习率策略
- 权重初始化
- 剪枝与量化
- 记录：
    - 训练技巧经验：

## 轻量化网络
- MobileNet系列
- ShuffleNet
- SqueezeNet
- GhostNet
- 模型压缩与加速
- 记录：
    - 轻量化方法比较：

## CNN在光学检测中的应用
- 表面缺陷检测
- 目标定位与测量
- 工件识别与分类
- 质量等级评估
- 部署考量
- 记录：
    - 光学检测应用案例：

## 工业级CNN实现
- 代码结构组织
- 数据流水线设计
- 验证与测试策略
- 模型版本管理
- 部署与推理优化
- 记录：
    - 工程实践经验：

## 学习资源与书签
- 经典论文与综述
- 在线课程与教程
- 开源实现与框架
- 行业应用案例
- 记录：
    - 资源链接：

## 学习进度与 TODO 列表
- [ ] 卷积原理掌握
- [ ] 经典CNN架构理解
- [ ] 现代CNN特点学习
- [ ] CNN训练实践
- [ ] 光学检测应用探索
- 自定义进度记录区：

---

备注：每个章节下可按"要点 / 代码示例 / 习题 / 参考链接 / 个人笔记"五小节结构补充内容。