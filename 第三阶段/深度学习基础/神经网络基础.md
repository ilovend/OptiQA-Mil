# 神经网络基础学习笔记 — 框架

说明：本文件为学习神经网络基础的笔记框架，后续在对应小节补充要点、示例、习题与心得。

## 目录
- [人工神经元](#人工神经元)
- [感知机与多层感知机](#感知机与多层感知机)
- [前向传播](#前向传播)
- [激活函数](#激活函数)
- [损失函数](#损失函数)
- [反向传播算法](#反向传播算法)
- [优化算法](#优化算法)
- [正则化技术](#正则化技术)
- [神经网络架构](#神经网络架构)
- [初始化策略](#初始化策略)
- [神经网络训练技巧](#神经网络训练技巧)
- [神经网络实现](#神经网络实现)
- [学习资源与书签](#学习资源与书签)
- [学习进度与 TODO 列表](#学习进度与-todo-列表)

---

## 人工神经元
- 生物神经元与人工神经元
- 神经元数学模型
- 权重与偏置
- 线性加权和与非线性变换
- 历史发展与里程碑
- 示例：
```python
# 单个神经元实现示例
import numpy as np
import matplotlib.pyplot as plt

class Neuron:
    def __init__(self, weights, bias):
        self.weights = weights
        self.bias = bias
        
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))
    
    def forward(self, inputs):
        # 线性组合
        linear_output = np.dot(self.weights, inputs) + self.bias
        # 通过激活函数
        return self.sigmoid(linear_output)

# 创建神经元实例
weights = np.array([0.5, -0.5, 0.3])  # 三个输入的权重
bias = -0.2
neuron = Neuron(weights, bias)

# 生成测试数据
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = np.zeros_like(X)

# 计算神经元在不同输入下的输出
# 固定第三个输入为1.0
for i in range(X.shape[0]):
    for j in range(X.shape[1]):
        Z[i, j] = neuron.forward(np.array([X[i, j], Y[i, j], 1.0]))

# 可视化神经元的输出
plt.figure(figsize=(10, 8))
cp = plt.contourf(X, Y, Z, cmap='viridis')
plt.colorbar(cp, label='神经元输出')
plt.title('单个神经元的输出响应面')
plt.xlabel('输入 1')
plt.ylabel('输入 2')
plt.grid(True)
plt.show()

# 查看决策边界（输出为0.5的等高线）
plt.figure(figsize=(10, 8))
plt.contour(X, Y, Z, levels=[0.5], colors='red', linewidths=2)
plt.title('神经元的决策边界 (输出=0.5)')
plt.xlabel('输入 1')
plt.ylabel('输入 2')
plt.grid(True)
plt.show()
```
- 记录：
    - 神经元模型理解要点：

## 感知机与多层感知机
- 单层感知机模型
- 多层感知机(MLP)架构
- 感知机的表示能力
- 通用近似定理
- XOR问题与线性可分性
- 记录：
    - 多层感知机特性：

## 前向传播
- 神经网络层的组织
- 矩阵表示与计算
- 层间信息传递
- 前向传播算法
- 计算图表示
- 记录：
    - 前向计算优化：

## 激活函数
- Sigmoid函数
- Tanh函数
- ReLU及其变种
- Softmax函数
- 激活函数的选择原则
- 记录：
    - 激活函数比较：

## 损失函数
- 均方误差(MSE)
- 交叉熵损失
- Focal Loss
- Huber Loss
- 损失函数的选择
- 记录：
    - 损失函数适用场景：

## 反向传播算法
- 梯度下降法原理
- 链式法则
- 误差反向传播过程
- 梯度计算
- 反向传播的实现
- 记录：
    - 反向传播理解要点：

## 优化算法
- 随机梯度下降(SGD)
- 动量法
- AdaGrad、RMSProp
- Adam优化器
- 学习率调度策略
- 记录：
    - 优化器选择经验：

## 正则化技术
- L1/L2正则化
- Dropout
- Batch Normalization
- 数据增强
- Early Stopping
- 记录：
    - 正则化效果比较：

## 神经网络架构
- 全连接网络
- 深度网络设计
- 跳跃连接
- 残差网络思想
- 架构设计原则
- 记录：
    - 网络设计心得：

## 初始化策略
- 零初始化问题
- 随机初始化
- Xavier/Glorot初始化
- He初始化
- 初始化对训练的影响
- 记录：
    - 初始化选择依据：

## 神经网络训练技巧
- 批处理与小批量训练
- 学习率选择
- 梯度裁剪
- 超参数调优
- 训练与验证策略
- 记录：
    - 训练经验总结：

## 神经网络实现
- 从零实现简单网络
- PyTorch/TensorFlow实现
- 代码组织与结构
- 调试与可视化
- 性能优化
- 记录：
    - 实现技巧：

## 学习资源与书签
- 经典教材与参考书
- 在线课程
- 研究论文
- 工具与框架
- 记录：
    - 资源链接：

## 学习进度与 TODO 列表
- [ ] 神经元模型理解
- [ ] 前向与反向传播掌握
- [ ] 激活函数与损失函数理解
- [ ] 优化算法实践
- [ ] 简单神经网络实现
- 自定义进度记录区：

---

备注：每个章节下可按"要点 / 代码示例 / 习题 / 参考链接 / 个人笔记"五小节结构补充内容。